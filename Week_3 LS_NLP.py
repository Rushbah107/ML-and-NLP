# -*- coding: utf-8 -*-
"""Week3 LS_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Emhzkc7wh9i-IxDR3M-mqrOy3lZsvmn7
"""

!pip install -U transformers

!pip install -U datasets fsspec

import transformers
print(transformers.__version__)

from transformers import pipeline
classifier = pipeline("sentiment-analysis")   # This pipeline uses pretrained model of 'Hugging Faces'
print(classifier("I love Hugging Face!"))  # [{'label': 'POSITIVE', 'score': 0.999}]

from datasets import load_dataset
dataset = load_dataset("imdb")    # It's a helper function from Hugging Face that downloads dataset, it's open source
print(dataset["train"][0])

from transformers import AutoTokenizer    # Autotokenizer: loads the correct tokenizer for model.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")    # Download and loads pre-trained tokenizer that have lowercase text
tokens = tokenizer("Hello, Hugging Face!", return_tensors="pt")   # "pt": PyTorch tensor
print(tokens)

from transformers import AutoModelForSequenceClassification, AutoTokenizer
model_name = "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

from transformers import pipeline
classifier = pipeline("sentiment-analysis", model=model_name)
print(classifier("Great tutorial!"))

classifier = pipeline("text-classification")
print(classifier("Fun movie!"))

def tokenize(examples):
  return tokenizer(examples["text"], padding="max_length", truncation=True)
tokenized_dataset = dataset.map(tokenize, batched=True)

tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset.set_format("torch")

from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    num_train_epochs=3,
)
trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset["train"], eval_dataset=tokenized_dataset["test"])
trainer.train()

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted")
    return {"accuracy": acc, "f1": f1}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics
)

print(trainer.evaluate())

"""## Explanation

In this machine learning pipeline, I fine-tune a pre-trained BERT-based model using Hugging Face's transformers and datasets libraries to perform sentiment analysis on the IMDb dataset. The pipeline started by loading the IMDb dataset with the datasets library, which provides easy access to standardized splits for training and evaluation. Next, the text data is tokenized using the tokenizer for bert-base-uncased, ensuring that inputs match the format the pre-trained model expects. The fine-tuning step leverages Trainer and TrainingArguments to set up training parameters such as batch size, learning rate, and evaluation strategy. After training, the model's performance is evaluated using metrics like accuracy and F1-score to gauge how well it distinguishes positive and negative sentiment. Finally, the fine-tuned model and tokenizer are push on Hugging Face enabling easy reuse or deployment.

Link of my model: "https://huggingface.co/Rushabh107/imdb-distilbert"
"""